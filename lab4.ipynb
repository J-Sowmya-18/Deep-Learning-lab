{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNf/CDOI4VvN1ZsIVg+C1q9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/J-Sowmya-18/Deep-Learning-lab/blob/main/lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Reshape\n",
        "\n",
        "# Function to generate training data for matrix multiplication\n",
        "def generate_data(num_samples):\n",
        "    matrices_a = np.random.randint(0, 10, (num_samples, 2, 2))\n",
        "    matrices_b = np.random.randint(0, 10, (num_samples, 2, 2))\n",
        "    matrices_c = np.matmul(matrices_a, matrices_b)\n",
        "    return (matrices_a, matrices_b), matrices_c\n",
        "\n",
        "num_samples = 10000\n",
        "(train_matrices_a, train_matrices_b), train_matrices_c = generate_data(num_samples)\n",
        "(test_matrices_a, test_matrices_b), test_matrices_c = generate_data(1000)\n",
        "\n",
        "# Reshape input matrices to fit the model\n",
        "train_inputs = np.concatenate([train_matrices_a.reshape(num_samples, 4), train_matrices_b.reshape(num_samples, 4)], axis=1)\n",
        "test_inputs = np.concatenate([test_matrices_a.reshape(1000, 4), test_matrices_b.reshape(1000, 4)], axis=1)\n",
        "train_outputs = train_matrices_c.reshape(num_samples, 4)\n",
        "test_outputs = test_matrices_c.reshape(1000, 4)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=8, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(4))  # Output layer for 2x2 matrix flattened\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_inputs, train_outputs, epochs=50, batch_size=64, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mae = model.evaluate(test_inputs, test_outputs)\n",
        "print(f'Test Loss: {loss}, Test MAE: {mae}')\n",
        "\n",
        "# Make predictions\n",
        "sample_a = np.array([[1, 2], [3, 4]])\n",
        "sample_b = np.array([[5, 6], [7, 8]])\n",
        "sample_input = np.concatenate([sample_a.flatten(), sample_b.flatten()]).reshape(1, -1)\n",
        "predicted_c = model.predict(sample_input).reshape(2, 2)\n",
        "\n",
        "print(f'Sample A:\\n{sample_a}')\n",
        "print(f'Sample B:\\n{sample_b}')\n",
        "print(f'Predicted Product:\\n{predicted_c}')\n",
        "print(f'Actual Product:\\n{np.matmul(sample_a, sample_b)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDuJACEDHVJX",
        "outputId": "35c53e6f-efc7-4155-90bd-74c4ddc565b4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1494.9755 - mae: 29.5576 - val_loss: 428.4759 - val_mae: 16.8526\n",
            "Epoch 2/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 346.2899 - mae: 15.0009 - val_loss: 192.7488 - val_mae: 11.0116\n",
            "Epoch 3/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 167.5358 - mae: 10.2254 - val_loss: 135.0065 - val_mae: 9.1631\n",
            "Epoch 4/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 119.5785 - mae: 8.6464 - val_loss: 95.6110 - val_mae: 7.6905\n",
            "Epoch 5/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 86.5353 - mae: 7.4076 - val_loss: 73.9699 - val_mae: 6.7806\n",
            "Epoch 6/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.0377 - mae: 6.6621 - val_loss: 60.3343 - val_mae: 6.0446\n",
            "Epoch 7/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 56.1261 - mae: 5.8950 - val_loss: 49.2416 - val_mae: 5.4583\n",
            "Epoch 8/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 46.4925 - mae: 5.3326 - val_loss: 40.9819 - val_mae: 4.9421\n",
            "Epoch 9/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 38.3049 - mae: 4.8106 - val_loss: 35.1866 - val_mae: 4.4955\n",
            "Epoch 10/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 32.4872 - mae: 4.3879 - val_loss: 30.1383 - val_mae: 4.1580\n",
            "Epoch 11/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27.9235 - mae: 4.0617 - val_loss: 26.6181 - val_mae: 3.9303\n",
            "Epoch 12/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 25.2960 - mae: 3.8354 - val_loss: 23.7547 - val_mae: 3.7276\n",
            "Epoch 13/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 22.6066 - mae: 3.6404 - val_loss: 22.1195 - val_mae: 3.6457\n",
            "Epoch 14/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 20.7387 - mae: 3.5033 - val_loss: 20.1823 - val_mae: 3.4455\n",
            "Epoch 15/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 19.7073 - mae: 3.4058 - val_loss: 19.8405 - val_mae: 3.4849\n",
            "Epoch 16/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 18.2354 - mae: 3.2795 - val_loss: 17.6974 - val_mae: 3.2220\n",
            "Epoch 17/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16.8995 - mae: 3.1422 - val_loss: 16.8017 - val_mae: 3.1604\n",
            "Epoch 18/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15.9566 - mae: 3.0749 - val_loss: 15.7955 - val_mae: 3.0675\n",
            "Epoch 19/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14.9834 - mae: 2.9861 - val_loss: 14.9308 - val_mae: 2.9935\n",
            "Epoch 20/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14.2520 - mae: 2.9084 - val_loss: 14.8335 - val_mae: 2.9563\n",
            "Epoch 21/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.2791 - mae: 2.8128 - val_loss: 13.9888 - val_mae: 2.8883\n",
            "Epoch 22/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 13.6033 - mae: 2.8492 - val_loss: 13.1836 - val_mae: 2.8172\n",
            "Epoch 23/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 12.6936 - mae: 2.7535 - val_loss: 12.9153 - val_mae: 2.7670\n",
            "Epoch 24/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 11.9279 - mae: 2.6710 - val_loss: 12.2968 - val_mae: 2.7250\n",
            "Epoch 25/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 11.2289 - mae: 2.6040 - val_loss: 11.6711 - val_mae: 2.6483\n",
            "Epoch 26/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.8671 - mae: 2.5535 - val_loss: 11.3983 - val_mae: 2.6258\n",
            "Epoch 27/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.4011 - mae: 2.5047 - val_loss: 11.5298 - val_mae: 2.6509\n",
            "Epoch 28/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.2132 - mae: 2.4805 - val_loss: 10.5135 - val_mae: 2.5219\n",
            "Epoch 29/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.6643 - mae: 2.4222 - val_loss: 10.5105 - val_mae: 2.5013\n",
            "Epoch 30/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.1801 - mae: 2.3599 - val_loss: 9.9421 - val_mae: 2.4425\n",
            "Epoch 31/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.0248 - mae: 2.3478 - val_loss: 9.6380 - val_mae: 2.4017\n",
            "Epoch 32/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.9521 - mae: 2.3354 - val_loss: 9.4055 - val_mae: 2.3730\n",
            "Epoch 33/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.6228 - mae: 2.3004 - val_loss: 8.8572 - val_mae: 2.3070\n",
            "Epoch 34/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.2625 - mae: 2.2507 - val_loss: 9.2846 - val_mae: 2.3778\n",
            "Epoch 35/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.2530 - mae: 2.2442 - val_loss: 8.9054 - val_mae: 2.3223\n",
            "Epoch 36/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1642 - mae: 2.2298 - val_loss: 8.8131 - val_mae: 2.3002\n",
            "Epoch 37/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.7370 - mae: 2.1818 - val_loss: 7.9758 - val_mae: 2.1952\n",
            "Epoch 38/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.3730 - mae: 2.1258 - val_loss: 7.8842 - val_mae: 2.1770\n",
            "Epoch 39/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1922 - mae: 2.0964 - val_loss: 7.5861 - val_mae: 2.1337\n",
            "Epoch 40/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.0015 - mae: 2.0762 - val_loss: 7.4622 - val_mae: 2.1165\n",
            "Epoch 41/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.0571 - mae: 2.0861 - val_loss: 7.2514 - val_mae: 2.0950\n",
            "Epoch 42/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.7318 - mae: 2.0338 - val_loss: 7.0737 - val_mae: 2.0550\n",
            "Epoch 43/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.5411 - mae: 1.9996 - val_loss: 7.0159 - val_mae: 2.0422\n",
            "Epoch 44/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.5088 - mae: 1.9928 - val_loss: 6.9469 - val_mae: 2.0395\n",
            "Epoch 45/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.1895 - mae: 1.9463 - val_loss: 6.7406 - val_mae: 2.0091\n",
            "Epoch 46/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9581 - mae: 1.9100 - val_loss: 6.4707 - val_mae: 1.9726\n",
            "Epoch 47/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.7632 - mae: 1.8845 - val_loss: 6.2606 - val_mae: 1.9460\n",
            "Epoch 48/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.7418 - mae: 1.8675 - val_loss: 6.1334 - val_mae: 1.9274\n",
            "Epoch 49/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5685 - mae: 1.8497 - val_loss: 6.1213 - val_mae: 1.9295\n",
            "Epoch 50/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.5050 - mae: 1.8446 - val_loss: 5.8153 - val_mae: 1.8722\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.9580 - mae: 1.8919 \n",
            "Test Loss: 5.737789154052734, Test MAE: 1.8645330667495728\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "Sample A:\n",
            "[[1 2]\n",
            " [3 4]]\n",
            "Sample B:\n",
            "[[5 6]\n",
            " [7 8]]\n",
            "Predicted Product:\n",
            "[[17.60358  19.682005]\n",
            " [44.086758 50.434956]]\n",
            "Actual Product:\n",
            "[[19 22]\n",
            " [43 50]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Reshape\n",
        "\n",
        "# Function to generate training data for matrix multiplication\n",
        "def generate_data(num_samples):\n",
        "    matrices_a = np.random.randint(0, 10, (num_samples, 2, 2))\n",
        "    matrices_b = np.random.randint(0, 10, (num_samples, 2, 2))\n",
        "    matrices_c = np.matmul(matrices_a, matrices_b)\n",
        "    return (matrices_a, matrices_b), matrices_c\n",
        "\n",
        "num_samples = 10000\n",
        "(train_matrices_a, train_matrices_b), train_matrices_c = generate_data(num_samples)\n",
        "(test_matrices_a, test_matrices_b), test_matrices_c = generate_data(1000)\n",
        "\n",
        "# Reshape input matrices to fit the model\n",
        "train_inputs = np.concatenate([train_matrices_a.reshape(num_samples, 4), train_matrices_b.reshape(num_samples, 4)], axis=1)\n",
        "test_inputs = np.concatenate([test_matrices_a.reshape(1000, 4), test_matrices_b.reshape(1000, 4)], axis=1)\n",
        "train_outputs = train_matrices_c.reshape(num_samples, 4)\n",
        "test_outputs = test_matrices_c.reshape(1000, 4)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=8, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(4))  # Output layer for 2x2 matrix flattened\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_inputs, train_outputs, epochs=50, batch_size=64, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mae = model.evaluate(test_inputs, test_outputs)\n",
        "print(f'Test Loss: {loss}, Test MAE: {mae}')\n",
        "\n",
        "# Make predictions\n",
        "sample_a = np.array([[5,6], [7, 8]])\n",
        "sample_b = np.array([[1,2], [3, 4]])\n",
        "sample_input = np.concatenate([sample_a.flatten(), sample_b.flatten()]).reshape(1, -1)\n",
        "predicted_c = model.predict(sample_input).reshape(2, 2)\n",
        "\n",
        "print(f'Sample A:\\n{sample_a}')\n",
        "print(f'Sample B:\\n{sample_b}')\n",
        "print(f'Predicted Product:\\n{predicted_c}')\n",
        "print(f'Actual Product:\\n{np.matmul(sample_a, sample_b)}')"
      ],
      "metadata": {
        "id": "f4CbG5YqHidZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}