{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQ5hAXzdXcd3goS2deu/ZU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/J-Sowmya-18/Deep-Learning-lab/blob/main/lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "haYE6glkEFYT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "307beced-b908-4a58-96da-b7c6763ef2da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1739.2028 - mae: 32.4897 - val_loss: 439.3103 - val_mae: 17.1742\n",
            "Epoch 2/15\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 348.1412 - mae: 15.1431 - val_loss: 192.9930 - val_mae: 11.0337\n",
            "Epoch 3/15\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 182.8674 - mae: 10.7278 - val_loss: 149.2894 - val_mae: 9.6125\n",
            "Epoch 4/15\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 137.1040 - mae: 9.2702 - val_loss: 106.1267 - val_mae: 8.1372\n",
            "Epoch 5/15\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 98.6276 - mae: 7.8448 - val_loss: 82.9118 - val_mae: 7.1548\n",
            "Epoch 6/15\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76.4562 - mae: 6.8595 - val_loss: 67.0471 - val_mae: 6.3476\n",
            "Epoch 7/15\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 60.6138 - mae: 6.0833 - val_loss: 52.6476 - val_mae: 5.6838\n",
            "Epoch 8/15\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 48.3432 - mae: 5.4255 - val_loss: 42.1392 - val_mae: 5.0790\n",
            "Epoch 9/15\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 37.8083 - mae: 4.7722 - val_loss: 34.7136 - val_mae: 4.6632\n",
            "Epoch 10/15\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31.9997 - mae: 4.3934 - val_loss: 28.0302 - val_mae: 4.0334\n",
            "Epoch 11/15\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 26.3207 - mae: 3.9665 - val_loss: 23.6649 - val_mae: 3.7267\n",
            "Epoch 12/15\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 23.0755 - mae: 3.6993 - val_loss: 21.4352 - val_mae: 3.5141\n",
            "Epoch 13/15\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 20.7716 - mae: 3.5075 - val_loss: 19.3577 - val_mae: 3.3503\n",
            "Epoch 14/15\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 18.4488 - mae: 3.2967 - val_loss: 18.1026 - val_mae: 3.2750\n",
            "Epoch 15/15\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17.2659 - mae: 3.1961 - val_loss: 16.6022 - val_mae: 3.1226\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 16.3016 - mae: 3.1337 \n",
            "Test Loss: 16.5799617767334, Test MAE: 3.1310930252075195\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "Sample A:\n",
            "[[4 3]\n",
            " [2 1]]\n",
            "Sample B:\n",
            "[[1 3]\n",
            " [5 7]]\n",
            "Predicted Product:\n",
            "[[21.281261 44.5085  ]\n",
            " [ 9.529257 13.907981]]\n",
            "Actual Product:\n",
            "[[19 33]\n",
            " [ 7 13]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Reshape\n",
        "\n",
        "# Function to generate training data for matrix multiplication\n",
        "def generate_data(num_samples):\n",
        "    matrices_a = np.random.randint(0, 10, (num_samples, 2, 2))\n",
        "    matrices_b = np.random.randint(0, 10, (num_samples, 2, 2))\n",
        "    matrices_c = np.matmul(matrices_a, matrices_b)\n",
        "    return (matrices_a, matrices_b), matrices_c\n",
        "\n",
        "num_samples = 10000\n",
        "(train_matrices_a, train_matrices_b), train_matrices_c = generate_data(num_samples)\n",
        "(test_matrices_a, test_matrices_b), test_matrices_c = generate_data(1000)\n",
        "\n",
        "# Reshape input matrices to fit the model\n",
        "train_inputs = np.concatenate([train_matrices_a.reshape(num_samples, 4), train_matrices_b.reshape(num_samples, 4)], axis=1)\n",
        "test_inputs = np.concatenate([test_matrices_a.reshape(1000, 4), test_matrices_b.reshape(1000, 4)], axis=1)\n",
        "train_outputs = train_matrices_c.reshape(num_samples, 4)\n",
        "test_outputs = test_matrices_c.reshape(1000, 4)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=8, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(4))  # Output layer for 2x2 matrix flattened\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_inputs, train_outputs, epochs=15, batch_size=64, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mae = model.evaluate(test_inputs, test_outputs)\n",
        "print(f'Test Loss: {loss}, Test MAE: {mae}')\n",
        "\n",
        "# Make predictions\n",
        "sample_a = np.array([[4 , 3], [2 , 1]])\n",
        "sample_b = np.array([[1 , 3], [5, 7]])\n",
        "sample_input = np.concatenate([sample_a.flatten(), sample_b.flatten()]).reshape(1, -1)\n",
        "predicted_c = model.predict(sample_input).reshape(2, 2)\n",
        "\n",
        "print(f'Sample A:\\n{sample_a}')\n",
        "print(f'Sample B:\\n{sample_b}')\n",
        "print(f'Predicted Product:\\n{predicted_c}')\n",
        "print(f'Actual Product:\\n{np.matmul(sample_a, sample_b)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Reshape\n",
        "\n",
        "# Function to generate training data for matrix multiplication\n",
        "def generate_data(num_samples):\n",
        "    matrices_a = np.random.randint(0, 10, (num_samples, 2, 2))\n",
        "    matrices_b = np.random.randint(0, 10, (num_samples, 2, 2))\n",
        "    matrices_c = np.matmul(matrices_a, matrices_b)\n",
        "    return (matrices_a, matrices_b), matrices_c\n",
        "\n",
        "num_samples = 10000\n",
        "(train_matrices_a, train_matrices_b), train_matrices_c = generate_data(num_samples)\n",
        "(test_matrices_a, test_matrices_b), test_matrices_c = generate_data(1000)\n",
        "\n",
        "# Reshape input matrices to fit the model\n",
        "train_inputs = np.concatenate([train_matrices_a.reshape(num_samples, 4), train_matrices_b.reshape(num_samples, 4)], axis=1)\n",
        "test_inputs = np.concatenate([test_matrices_a.reshape(1000, 4), test_matrices_b.reshape(1000, 4)], axis=1)\n",
        "train_outputs = train_matrices_c.reshape(num_samples, 4)\n",
        "test_outputs = test_matrices_c.reshape(1000, 4)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=8, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(4))  # Output layer for 2x2 matrix flattened\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_inputs, train_outputs, epochs=50, batch_size=64, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mae = model.evaluate(test_inputs, test_outputs)\n",
        "print(f'Test Loss: {loss}, Test MAE: {mae}')\n",
        "\n",
        "# Make predictions\n",
        "sample_a = np.array([[1, 2], [3, 4]])\n",
        "sample_b = np.array([[5, 6], [7, 8]])\n",
        "sample_input = np.concatenate([sample_a.flatten(), sample_b.flatten()]).reshape(1, -1)\n",
        "predicted_c = model.predict(sample_input).reshape(2, 2)\n",
        "\n",
        "print(f'Sample A:\\n{sample_a}')\n",
        "print(f'Sample B:\\n{sample_b}')\n",
        "print(f'Predicted Product:\\n{predicted_c}')\n",
        "print(f'Actual Product:\\n{np.matmul(sample_a, sample_b)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNhCUShTKMsP",
        "outputId": "da23b80b-c214-482e-9fa9-38ee88fa68ca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1492.1239 - mae: 29.6360 - val_loss: 441.9550 - val_mae: 17.1931\n",
            "Epoch 2/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 363.2869 - mae: 15.3313 - val_loss: 177.4426 - val_mae: 10.5885\n",
            "Epoch 3/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 156.3324 - mae: 9.8945 - val_loss: 121.6147 - val_mae: 8.7628\n",
            "Epoch 4/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 109.5209 - mae: 8.2656 - val_loss: 87.0859 - val_mae: 7.3348\n",
            "Epoch 5/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 81.6676 - mae: 7.1352 - val_loss: 68.2796 - val_mae: 6.5031\n",
            "Epoch 6/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 64.7855 - mae: 6.3485 - val_loss: 55.7505 - val_mae: 5.8516\n",
            "Epoch 7/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 52.8995 - mae: 5.6884 - val_loss: 44.4729 - val_mae: 5.2430\n",
            "Epoch 8/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 42.2804 - mae: 5.0661 - val_loss: 36.1310 - val_mae: 4.6540\n",
            "Epoch 9/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 34.8273 - mae: 4.5611 - val_loss: 30.2236 - val_mae: 4.1796\n",
            "Epoch 10/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 28.4213 - mae: 4.0993 - val_loss: 25.2294 - val_mae: 3.8461\n",
            "Epoch 11/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 24.2628 - mae: 3.8202 - val_loss: 22.0326 - val_mae: 3.6209\n",
            "Epoch 12/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.2206 - mae: 3.5349 - val_loss: 19.2781 - val_mae: 3.3615\n",
            "Epoch 13/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 18.5612 - mae: 3.2937 - val_loss: 17.1426 - val_mae: 3.1616\n",
            "Epoch 14/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16.7760 - mae: 3.1315 - val_loss: 16.0154 - val_mae: 3.0380\n",
            "Epoch 15/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15.1918 - mae: 2.9894 - val_loss: 14.5656 - val_mae: 2.9359\n",
            "Epoch 16/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 14.0263 - mae: 2.8696 - val_loss: 13.5781 - val_mae: 2.8432\n",
            "Epoch 17/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.9428 - mae: 2.7530 - val_loss: 12.4639 - val_mae: 2.7053\n",
            "Epoch 18/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 11.9280 - mae: 2.6524 - val_loss: 11.9736 - val_mae: 2.6584\n",
            "Epoch 19/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.4820 - mae: 2.6093 - val_loss: 11.1963 - val_mae: 2.5740\n",
            "Epoch 20/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.6893 - mae: 2.5163 - val_loss: 10.8013 - val_mae: 2.5393\n",
            "Epoch 21/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.4433 - mae: 2.4973 - val_loss: 10.7241 - val_mae: 2.5308\n",
            "Epoch 22/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.8241 - mae: 2.4278 - val_loss: 9.7108 - val_mae: 2.3991\n",
            "Epoch 23/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.3764 - mae: 2.3669 - val_loss: 10.2277 - val_mae: 2.4735\n",
            "Epoch 24/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.2009 - mae: 2.3543 - val_loss: 9.4812 - val_mae: 2.3877\n",
            "Epoch 25/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.6206 - mae: 2.2733 - val_loss: 9.1474 - val_mae: 2.3428\n",
            "Epoch 26/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.2897 - mae: 2.2393 - val_loss: 8.3448 - val_mae: 2.2447\n",
            "Epoch 27/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.1177 - mae: 2.2137 - val_loss: 7.9322 - val_mae: 2.1816\n",
            "Epoch 28/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.6818 - mae: 2.1528 - val_loss: 7.8446 - val_mae: 2.1770\n",
            "Epoch 29/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.4681 - mae: 2.1245 - val_loss: 7.6586 - val_mae: 2.1581\n",
            "Epoch 30/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.2134 - mae: 2.0915 - val_loss: 7.6175 - val_mae: 2.1495\n",
            "Epoch 31/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6.8852 - mae: 2.0509 - val_loss: 7.1472 - val_mae: 2.0770\n",
            "Epoch 32/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6.8093 - mae: 2.0343 - val_loss: 6.8626 - val_mae: 2.0408\n",
            "Epoch 33/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6.5291 - mae: 1.9930 - val_loss: 6.6344 - val_mae: 2.0047\n",
            "Epoch 34/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6.2758 - mae: 1.9546 - val_loss: 6.3549 - val_mae: 1.9581\n",
            "Epoch 35/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.0028 - mae: 1.9160 - val_loss: 6.3688 - val_mae: 1.9639\n",
            "Epoch 36/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.0276 - mae: 1.9216 - val_loss: 6.0958 - val_mae: 1.9203\n",
            "Epoch 37/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.8052 - mae: 1.8841 - val_loss: 6.0579 - val_mae: 1.9137\n",
            "Epoch 38/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.7723 - mae: 1.8789 - val_loss: 5.7028 - val_mae: 1.8612\n",
            "Epoch 39/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5266 - mae: 1.8358 - val_loss: 5.5791 - val_mae: 1.8418\n",
            "Epoch 40/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2437 - mae: 1.7923 - val_loss: 5.5401 - val_mae: 1.8386\n",
            "Epoch 41/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.2796 - mae: 1.7875 - val_loss: 5.2065 - val_mae: 1.7719\n",
            "Epoch 42/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.9429 - mae: 1.7355 - val_loss: 5.1432 - val_mae: 1.7665\n",
            "Epoch 43/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8502 - mae: 1.7184 - val_loss: 5.1083 - val_mae: 1.7671\n",
            "Epoch 44/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7817 - mae: 1.7103 - val_loss: 5.0114 - val_mae: 1.7500\n",
            "Epoch 45/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5033 - mae: 1.6622 - val_loss: 4.9014 - val_mae: 1.7252\n",
            "Epoch 46/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4443 - mae: 1.6486 - val_loss: 5.0756 - val_mae: 1.7516\n",
            "Epoch 47/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6199 - mae: 1.6875 - val_loss: 4.6290 - val_mae: 1.6744\n",
            "Epoch 48/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3880 - mae: 1.6460 - val_loss: 4.4566 - val_mae: 1.6460\n",
            "Epoch 49/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4006 - mae: 1.6479 - val_loss: 4.3373 - val_mae: 1.6152\n",
            "Epoch 50/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2719 - mae: 1.6169 - val_loss: 4.4387 - val_mae: 1.6427\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1709 - mae: 1.5970 \n",
            "Test Loss: 4.3808979988098145, Test MAE: 1.645787000656128\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "Sample A:\n",
            "[[1 2]\n",
            " [3 4]]\n",
            "Sample B:\n",
            "[[5 6]\n",
            " [7 8]]\n",
            "Predicted Product:\n",
            "[[16.59826  20.128748]\n",
            " [44.57039  48.51153 ]]\n",
            "Actual Product:\n",
            "[[19 22]\n",
            " [43 50]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prime numbers\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Function to determine if a number is prime\n",
        "def is_prime(n):\n",
        "    if n <= 1:\n",
        "        return 0\n",
        "    for i in range(2, int(np.sqrt(n)) + 1):\n",
        "        if n % i == 0:\n",
        "            return 0\n",
        "    return 1\n",
        "\n",
        "# Generate data for numbers 0 through 9999\n",
        "numbers = np.arange(10000)\n",
        "labels = np.array([is_prime(n) for n in numbers])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(numbers, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the data (optional, but can help with training)\n",
        "X_train = X_train / 10000.0\n",
        "X_test = X_test / 10000.0\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=1, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
        "\n",
        "# Make predictions\n",
        "sample_numbers = np.array([2, 5, 19, 20]) / 10000.0  # Normalizing the sample numbers\n",
        "predictions = model.predict(sample_numbers)\n",
        "\n",
        "# Print predictions\n",
        "for i, num in enumerate([2, 5, 19, 20]):\n",
        "    print(f'Number: {num}, Is Prime (Predicted): {round(predictions[i][0])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5bxPOz5KVeX",
        "outputId": "43e9e5ce-2963-4e9a-8d48-87098752b132"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7720 - loss: 0.5958 - val_accuracy: 0.8869 - val_loss: 0.3635\n",
            "Epoch 2/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8790 - loss: 0.3803 - val_accuracy: 0.8869 - val_loss: 0.3602\n",
            "Epoch 3/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8790 - loss: 0.3694 - val_accuracy: 0.8869 - val_loss: 0.3585\n",
            "Epoch 4/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8783 - loss: 0.3732 - val_accuracy: 0.8869 - val_loss: 0.3555\n",
            "Epoch 5/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8762 - loss: 0.3738 - val_accuracy: 0.8869 - val_loss: 0.3547\n",
            "Epoch 6/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8736 - loss: 0.3809 - val_accuracy: 0.8869 - val_loss: 0.3538\n",
            "Epoch 7/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8683 - loss: 0.3895 - val_accuracy: 0.8869 - val_loss: 0.3533\n",
            "Epoch 8/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8722 - loss: 0.3824 - val_accuracy: 0.8869 - val_loss: 0.3537\n",
            "Epoch 9/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8757 - loss: 0.3726 - val_accuracy: 0.8869 - val_loss: 0.3537\n",
            "Epoch 10/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8776 - loss: 0.3719 - val_accuracy: 0.8869 - val_loss: 0.3532\n",
            "Epoch 11/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8793 - loss: 0.3683 - val_accuracy: 0.8869 - val_loss: 0.3541\n",
            "Epoch 12/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8734 - loss: 0.3796 - val_accuracy: 0.8869 - val_loss: 0.3534\n",
            "Epoch 13/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8769 - loss: 0.3723 - val_accuracy: 0.8869 - val_loss: 0.3530\n",
            "Epoch 14/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8743 - loss: 0.3770 - val_accuracy: 0.8869 - val_loss: 0.3526\n",
            "Epoch 15/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8775 - loss: 0.3705 - val_accuracy: 0.8869 - val_loss: 0.3540\n",
            "Epoch 16/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8777 - loss: 0.3713 - val_accuracy: 0.8869 - val_loss: 0.3526\n",
            "Epoch 17/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8750 - loss: 0.3755 - val_accuracy: 0.8869 - val_loss: 0.3528\n",
            "Epoch 18/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8768 - loss: 0.3710 - val_accuracy: 0.8869 - val_loss: 0.3547\n",
            "Epoch 19/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8780 - loss: 0.3712 - val_accuracy: 0.8869 - val_loss: 0.3531\n",
            "Epoch 20/20\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8800 - loss: 0.3659 - val_accuracy: 0.8869 - val_loss: 0.3529\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8721 - loss: 0.3814\n",
            "Test Loss: 0.38291600346565247, Test Accuracy: 0.8715000152587891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f12ff6044c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "Number: 2, Is Prime (Predicted): 0\n",
            "Number: 5, Is Prime (Predicted): 0\n",
            "Number: 19, Is Prime (Predicted): 0\n",
            "Number: 20, Is Prime (Predicted): 0\n"
          ]
        }
      ]
    }
  ]
}